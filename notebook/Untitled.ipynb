{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrow\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datagen.csvgen.base import randgen, randrec\n",
    "from datagen.csvgen.ner import converter\n",
    "from tqdm import tqdm\n",
    "from pandas.core.common import flatten\n",
    "from collections import OrderedDict\n",
    "from datagen.config import data_config\n",
    "\n",
    "from datagen.imgen.content import ner_utils\n",
    "from datagen.imgen.ops import boxes_ops\n",
    "\n",
    "\n",
    "import json\n",
    "import chardet\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label config\n",
    "\n",
    "def labels_map(label_name, label_type):\n",
    "    labels=[]\n",
    "    for kn,vn in label_name.items():\n",
    "        for kt,vt in label_type.items():\n",
    "            if kt!='delimiter':\n",
    "                for bil in \"BILU\":\n",
    "                    name = f'{bil}-{vt}_{vn}'\n",
    "                    labels.append(name)\n",
    "    labels.append(\"O\")\n",
    "    return labels\n",
    "\n",
    "\n",
    "label_name = {\n",
    "    'provinsi': 'PROV',\n",
    "    'kabupaten': 'KAB',\n",
    "    'nik': 'NIK',\n",
    "    'nama': 'NAMA',\n",
    "    'ttl': 'TTL',\n",
    "    'gender': 'GDR',\n",
    "    'goldar': 'GLD',\n",
    "    'alamat': 'ADR',\n",
    "    'rtrw': 'RTW',\n",
    "    'kelurahan': 'KLH',\n",
    "    'kecamatan': 'KCM',\n",
    "    'agama': 'RLG',\n",
    "    'perkawinan': 'KWN',\n",
    "    'pekerjaan': 'KRJ',\n",
    "    'kewarganegaraan': 'WRG',\n",
    "    'berlaku': 'BLK',\n",
    "    'sign_place': 'SGP',\n",
    "    'sign_date': 'SGD'\n",
    "}\n",
    "\n",
    "label_type = {\n",
    "    'field': \"FLD\",\n",
    "    'value': \"VAL\",\n",
    "    \"delimiter\": \"O\"\n",
    "}\n",
    "\n",
    "LABELS = labels_map(label_name, label_type)\n",
    "LABEL2INDEX = dict((label,idx) for idx, label in enumerate(LABELS))\n",
    "INDEX2LABEL = dict((idx, label) for idx, label in enumerate(LABELS))\n",
    "NUM_LABELS = len(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_annotation_objects(anno, tokenizer, max_seq_length=512):\n",
    "    objects = anno['objects']\n",
    "    objects = prepare_objects_data(objects, tokenizer)\n",
    "    \n",
    "    tokens, labels, label_ids, boxes = [],[],[],[]\n",
    "    \n",
    "    tokens.append(cls_token)\n",
    "    boxes.append(cls_token_box)\n",
    "    label_ids.append(pad_token_label_id)\n",
    "    \n",
    "    for obj in objects:\n",
    "        tokens.append(obj['token'])\n",
    "        labels.append(obj['label'])\n",
    "\n",
    "        lab = LABEL2INDEX[obj['label']]\n",
    "        label_ids.append(lab)\n",
    "\n",
    "        pts = obj['points']\n",
    "        pts = np.array(pts)\n",
    "        pts = boxes_ops.order_points(np.array(pts))\n",
    "        pts = list(boxes_ops.to_xyminmax(pts))\n",
    "        boxes.append(pts)\n",
    "\n",
    "    tokens.append(sep_token)\n",
    "    boxes.append(sep_token_box)\n",
    "    label_ids.append(pad_token_label_id)\n",
    "\n",
    "    input_ids = tokenizer.encode(tokens, add_special_tokens=False)\n",
    "    input_masks = [1] * len(input_ids)\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "    \n",
    "    padding_data_ouput = padding_data(\n",
    "        input_ids, input_masks, \n",
    "        segment_ids, label_ids, boxes,\n",
    "        max_seq_length=max_seq_length\n",
    "    )\n",
    "    input_ids, input_masks, segment_ids, label_ids, boxes = padding_data_ouput\n",
    "    \n",
    "    return input_ids, input_masks, segment_ids, label_ids, boxes\n",
    "\n",
    "\n",
    "def padding_data(input_ids, input_masks, segment_ids,\n",
    "                label_ids, boxes, max_seq_length=512, \n",
    "                pad_on_left=False):\n",
    "    \n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "\n",
    "    if not pad_on_left:\n",
    "        input_ids += [pad_token] * padding_length\n",
    "        input_masks += [0] * padding_length\n",
    "        segment_ids += [0] * padding_length\n",
    "        label_ids += [pad_token_label_id] * padding_length\n",
    "        boxes += [pad_token_box] * padding_length\n",
    "    else:\n",
    "        input_ids = [pad_token] * padding_length + input_ids\n",
    "        input_masks = [0] * padding_length + input_masks\n",
    "        segment_ids = [0] * padding_length + segment_ids\n",
    "        label_ids = [pad_token_label_id] * padding_length + label_ids\n",
    "        boxes = [pad_token_box] * padding_length + boxes\n",
    "        \n",
    "    return input_ids, input_masks, segment_ids, label_ids, boxes\n",
    "\n",
    "\n",
    "def prepare_objects_data(objects, tokenizer):\n",
    "    duplicated_objects = tokenize_duplicate_dict(objects, tokenizer)\n",
    "    formatted_objects = reformat_label_oriented(duplicated_objects)\n",
    "    bilou_objects = inject_bilou_to_objects(formatted_objects)\n",
    "    objects = revert_to_list_format(bilou_objects)\n",
    "    \n",
    "    return objects\n",
    "\n",
    "def reformat_label_oriented(objects):\n",
    "    data = OrderedDict({k:{'field':[], 'delimiter':[], 'value':[]} for k,v in label_name.items()})\n",
    "\n",
    "    for idx, obj in enumerate(objects):\n",
    "        cname_curr = obj['classname']\n",
    "        scname_curr = obj['subclass']\n",
    "        data[cname_curr][scname_curr].append(obj)\n",
    "\n",
    "    return data\n",
    "\n",
    "# datas['objects']\n",
    "def bilou_prefixer(text_list, label=None):\n",
    "    out = []\n",
    "    text_len = len(text_list)\n",
    "    if text_len==1:\n",
    "        bl = \"U\"\n",
    "        if label!=None: bl =  bl + \"-\" + label\n",
    "        out.append(bl)\n",
    "    elif text_len>1:\n",
    "        for idx, text in enumerate(text_list):\n",
    "            if idx==0: \n",
    "                bl = \"B\"\n",
    "                if label!=None: bl = bl + \"-\" + label\n",
    "                out.append(bl)\n",
    "            elif idx < text_len - 1: \n",
    "                bl = \"I\"\n",
    "                if label!=None: bl = bl + \"-\" + label\n",
    "                out.append(bl)\n",
    "            else: \n",
    "                bl = \"L\"\n",
    "                if label!=None: bl =  bl + \"-\" + label\n",
    "                out.append(bl)\n",
    "    return out\n",
    "\n",
    "\n",
    "def tokenize_inside_dict(data_dict, tokenizer):\n",
    "    for idx in range(len(data_dict)):\n",
    "        text = data_dict[idx]['text']\n",
    "        data_dict[idx]['text'] = tokenizer.tokenize(text)\n",
    "    return data_dict\n",
    "\n",
    "def tokenize_duplicate_dict(objects, tokenizer):\n",
    "    new_objects = []\n",
    "    for idx, obj in enumerate(objects):\n",
    "        curr_text = objects[idx]['text']\n",
    "\n",
    "        token = tokenizer.tokenize(curr_text)\n",
    "        if len(token) > 1:\n",
    "            for tok in token:\n",
    "                new_obj = objects[idx].copy()\n",
    "                new_obj['token'] = tok\n",
    "                new_objects.append(new_obj)\n",
    "        else:\n",
    "            if len(token)==0:\n",
    "                obj['token'] = ''\n",
    "            else:\n",
    "                obj['token'] = token[0]\n",
    "            new_objects.append(obj)\n",
    "\n",
    "    return new_objects\n",
    "\n",
    "\n",
    "def inject_bilou_to_label(data_dict):\n",
    "    # create bilou prefix to dictionary data\n",
    "    texts = []\n",
    "    for idx in range(len(data_dict)):\n",
    "        texts.append(data_dict[idx]['token'])\n",
    "    bil_prefix = bilou_prefixer(texts)\n",
    "\n",
    "    #inject bilou prefix into label inside data_dict\n",
    "    for idx, (bil, fld) in enumerate(zip(bil_prefix, data_dict)):\n",
    "        if fld['label'] != \"O\":\n",
    "            label = bil+'-'+fld['label']\n",
    "            data_dict[idx]['label'] = label\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def inject_bilou_to_objects(objects):\n",
    "    for idx, (key,val) in enumerate(objects.items()):\n",
    "        field = val['field']\n",
    "        delim = val['delimiter']\n",
    "        value = val['value']\n",
    "\n",
    "        if len(field)>0:\n",
    "            objects[key]['field'] = inject_bilou_to_label(field)\n",
    "\n",
    "        if len(delim)>0:\n",
    "            objects[key]['delimiter'] = inject_bilou_to_label(delim)\n",
    "\n",
    "        if len(value)>0:\n",
    "            objects[key]['value'] = inject_bilou_to_label(value)\n",
    "    \n",
    "    return objects\n",
    "\n",
    "\n",
    "def revert_to_list_format(dnew):\n",
    "    data_list = []\n",
    "    for k,v in dnew.items():\n",
    "        field = dnew[k]['field']\n",
    "        delim = dnew[k]['delimiter']\n",
    "        value = dnew[k]['value']\n",
    "        if len(delim)>0:\n",
    "            line_list = field+delim+value\n",
    "        else:\n",
    "            line_list = field+value\n",
    "\n",
    "        data_list += line_list\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/combined/1606064001/7917_json.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path) as f:\n",
    "    datas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = prepdatas['objects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_at_end=False\n",
    "cls_token=\"[CLS]\"\n",
    "sep_token=\"[SEP]\"\n",
    "sep_token_extra=False\n",
    "\n",
    "cls_token_segment_id=1\n",
    "pad_token_segment_id=0\n",
    "sequence_a_segment_id=0\n",
    "\n",
    "pad_on_left=False\n",
    "pad_token=0\n",
    "\n",
    "cls_token_box=[0, 0, 0, 0]\n",
    "sep_token_box=[1500, 1500, 1500, 1500]\n",
    "pad_token_box=[0, 0, 0, 0]\n",
    "\n",
    "pad_token_label_id = nn.CrossEntropyLoss().ignore_index\n",
    "\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 0)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, input_masks, segment_ids, label_ids, boxes = build_data(objects, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(':', 'O', [346.0, 458.0, 364.0, 464.0])"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 17\n",
    "word = tokenizer.ids_to_tokens[input_ids[idx]]\n",
    "label = INDEX2LABEL[label_ids[idx]]\n",
    "word, label, boxes[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cls_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-0afd30018c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDCardDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# len(data[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-98-0afd30018c1f>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         anno_objects = get_formatted_annotation_objects(\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0manno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-454c2347ba7f>\u001b[0m in \u001b[0;36mget_formatted_annotation_objects\u001b[0;34m(anno, tokenizer, max_seq_length)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_token_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlabel_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_token_label_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cls_token' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class IDCardDataset(dataset.Dataset):\n",
    "    def __init__(self, root, tokenizer, labels=None, mode='train', \n",
    "                 test_size=0.2, max_seq_length=512):\n",
    "        self.root = Path(root)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        self.mode = mode\n",
    "        self.test_size = test_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self._build_files()\n",
    "        \n",
    "    def _build_files(self):\n",
    "        names = self._get_names(\"*_json.json\")\n",
    "        data = self._get_filtered_files(names)\n",
    "        dframe, train, test = self._split_dataset(data)\n",
    "        self.data_frame = dframe\n",
    "        self.train_frame = train\n",
    "        self.test_frame = test\n",
    "        \n",
    "        if self.mode==\"train\":\n",
    "            self.frame = self.train_frame\n",
    "        else:\n",
    "            self.frame = self.test_frame\n",
    "        \n",
    "    def _split_dataset(self, data):\n",
    "        dframe = pd.DataFrame(data)\n",
    "        train, test = train_test_split(dframe, \n",
    "                                       test_size=self.test_size, \n",
    "                                       random_state=1261)\n",
    "        train = train.reset_index(drop=True)\n",
    "        test = test.reset_index(drop=True)\n",
    "        return dframe, train, test\n",
    "        \n",
    "    def _get_filtered_files(self, names):\n",
    "        data = {'name':[], 'image':[], 'mask':[],'anno':[]}\n",
    "        \n",
    "        jfiles = self._glob_filter(\"*_json.json\")\n",
    "        ifiles = self._glob_filter(\"*_image.jpg\")\n",
    "        mfiles = self._glob_filter(\"*_mask.jpg\")\n",
    "        \n",
    "        for name in names:\n",
    "            for (jfile, ifile, mfile)  in zip(jfiles, ifiles, mfiles):\n",
    "                \n",
    "                jfpn = jfile.name.split(\"_\")[0]\n",
    "                ifpn = ifile.name.split(\"_\")[0]\n",
    "                mfpn = mfile.name.split(\"_\")[0]\n",
    "                \n",
    "                if name == jfpn and name == ifpn and name == mfpn:\n",
    "                    data['name'].append(name)\n",
    "                    data['image'].append(ifile)\n",
    "                    data['mask'].append(mfile)\n",
    "                    data['anno'].append(jfile)\n",
    "                    \n",
    "        return data\n",
    "\n",
    "    def _get_names(self, path_pattern):\n",
    "        names = []\n",
    "        files = self._glob_filter(path_pattern)\n",
    "        for file in files:\n",
    "            names.append(file.name.split(\"_\")[0])\n",
    "        return names\n",
    "    \n",
    "    def _glob_filter(self, pattern):\n",
    "        return sorted(list(self.root.glob(pattern)))\n",
    "    \n",
    "    def _load_anno(self, path):\n",
    "        path = str(path)\n",
    "        with open(path) as f:\n",
    "            data_dict = json.load(f)\n",
    "        return data_dict\n",
    "    \n",
    "    def _load_image(self, path):\n",
    "        path = str(path)\n",
    "        img = cv.imread(path, cv.IMREAD_UNCHANGED)\n",
    "        return img\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        record = self.frame.iloc[idx]\n",
    "        anno = self._load_anno(record['anno'])\n",
    "        img = self._load_image(record['image'])\n",
    "        mask = self._load_image(record['mask'])\n",
    "        \n",
    "        anno_objects = get_formatted_annotation_objects(\n",
    "            anno, self.tokenizer, self.max_seq_length\n",
    "        )\n",
    "        \n",
    "        input_ids, input_masks, segment_ids, label_ids, boxes = anno_objects\n",
    "        data = (\n",
    "            input_ids, input_masks, \n",
    "            segment_ids, label_ids, boxes,\n",
    "            img, mask\n",
    "        )\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    \n",
    "\n",
    "path = '../results/combined/1606064001/'\n",
    "data = IDCardDataset(root=path, tokenizer=tokenizer)\n",
    "# len(data[0])\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nunenuh/study/code/repo/idcard_datagen/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
